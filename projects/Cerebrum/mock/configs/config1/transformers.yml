application:
  # Main configuration for the application
  main_config:
    name: MyMainConfig1  # Name of main configuration
    python_executable: python3  # Python executable name
    n_python_workers: 32  # Number of Python processes to invoke
    use_cuda: yes  # Use CUDA for GPU acceleration if CUDA is available
    cuda_device: 0  # CUDA device ID to use (0 for the first GPU, 1 for the second, etc.)
    training_allowed: yes  # Allow model to train
    preload_model:
      enabled: no  # Preload a model or initialize a new one
      from: ./user_data/pretrained/model1.json  # File path to the model file
      error_on_inconsistent_model: no  # Error if the model parameters are inconsistent, otherwise, preload old parameters
      error_on_different_optimization: no  # Error if the optimization parameters are different, otherwise, train with new parameters
    save_checkpoints:
      enabled: yes  # Save model backups frequently
      to: ./data/output/  # Folder path to save checkpoints
      file_name_prefix: ~  # File name prefix for checkpoints ("~"" for default value, none)
      frequency: 120  # Delay between checkpoints (seconds)
    log:
      to: ./user_data/logs/  # Folder path to save logs
      file_name_prefix: ~  # File name prefix for logs ("~" for default value, none)
      n_rotating_logs: 10  # Number of rotating log files
      max_size_per_log: 10  # Maximum size per log files (MB)
      verbosity: low  # Verbosity level for logging (low/medium/high)
    intentional_delay_factor: 1  # Factor to delay entire model to run on low-end devices (1 for no delay, 2 for double delay, etc.)
    concurrent_reliability:
      training_cutoff: 0.9  # Cutoff for allowing training on low reliability (0-1, 0 to always train, 1 to never train unless perfect)
      warning_cutoff: 0.95  # Cutoff for warning about low reliabiliy (0-1, 0 to never warn, 1 to aalways warn)

  # Configuration for the behavior of the model
  model_config:
    random_seed: 42  # Random seed for deterministic random parameters
    n_levels: 4  # Number of levels of the model (Each level except the lowest one has subneurons)
    neuron_activity_control:
      goal: 0.1  # Target activity level for neurons (0-1, choosing a high value can cause CPU overload, depending on the numnber of levels)
      upward_effect:
        force: 0.2  # Force to increase the activity of neurons when goal is exceeded (0-1)
        easing: 0.5  # Easing factor for the upward force (0-1)
      downward_effect:
        force: 0.2  # Force to decrease the activity of neurons when goal is not reached (0-1)
        easing: 0.4  # Easing factor for the downward force (0-1)
    relay_neuron:
      transmission:
        synaptic_delay:
          min: 5  # Minimum delay for transmitting information between neurons (milliseconds)
          max: 20  # Maximum delay for transmitting information between neurons (milliseconds)
        max_message_length: 16  # Maximum length of messages transmitted between neurons (# of words)
        synaptic_transformer:  # Transformer model for deciding synaptic delay and whether to transmit to neuron or not
          types:  # List of transformer models to randomly select from (DistilBERT/TinyBERT/ALBERT/T5-Small/GPT2-Small)
            - DistilBERT
            - T5-Small
          weights:  # Frequencies for selecting each transformer model
            - 1  # DistilBERT
            - 1  # T5-Small
      memory_unit:
        capacity: 1024  # Maximum memory storage capacity for each neuron (# of characters)
        max_append_length: 5  # Maximum length of summary to append to memory for each neuron (# of words)
        memory_transformer:  # Tranformer model for summarizing messages to append to memory
          types:  # List of transformer models to randomly select from (DistilBERT/TinyBERT/ALBERT/T5-Small/GPT2-Small)
            - DistilBERT
            - T5-Small
          weights:  # Frequencies for selecting each transformer model
            - 1  # DistilBERT
            - 1  # T5-Small
    base_neuron:
      base_transformer:  # Transformer model for transforming base responses
        types:  # List of transformer models to randomly select from (GPT2/BERT/RoBERTA/T5/XLNET/BART)
          - GPT2
          - BERT
          - RoBERTA
          - T5
          - XLNET
          - BART
        weights:  # Frequencies for selecting each transformer model
          - 5  # GPT2
          - 5  # BERT
          - 2  # RoBERTA
          - 2  # T5
          - 2  # XLNET
          - 2  # BART
    response_neuron:
      confidence: 0.5 # Minimum confidence level to give a response (0-1)
      timeout: 20 # Timeout before giving most recent response (seconds)
      discriminator_transformer: GPT2  # Tranformer model for discriminating between responses (GPT2/BERT/RoBERTA/T5/XLNET/BART)

  # Configuration for the training process (if training is allowed)
  optimization_config:
    train_frequency: 1  # Frequency of training (# of responses)
    type: Adam  # Type of optimization algorithm (Adam/SGD/RMSProp/Adagrad)
    learning_rate: 0.001  # Learning rate for training
    alpha: ~ # Alpha parameter for training (0-1, "~" for default value, 0.9)
    beta: ~ # Beta parameter for training (0-1, "~" for default value, 0.999)
    batch_size: 32  # Batch size for training
    replay_memory: 1024  # Size of the replay memory (# of samples)